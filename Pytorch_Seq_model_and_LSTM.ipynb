{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch Seq model and LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bipinoli/AI_Adventure/blob/master/Pytorch_Seq_model_and_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmhFif47l157",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vem5LOau5je",
        "colab_type": "text"
      },
      "source": [
        "#Experimentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imFS4w6omhJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "655c25df-0834-47cc-b482-29f9c6abc7f9"
      },
      "source": [
        "lstm = nn.LSTM(input_size=3, hidden_size=4)\n",
        "print(lstm)\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "inputs = [torch.randn(1,3) for _ in range(5)]\n",
        "print(inputs)\n",
        "\n",
        "hidden = (torch.randn(1,1,4), torch.randn(1,1,4))\n",
        "original_hidden = hidden\n",
        "print(hidden)\n",
        "\n",
        "for i in inputs:\n",
        "  out, hidden = lstm(i.view(1,1,-1), hidden)\n",
        "  print(\"short+long memory\", hidden)\n",
        "  print(\"short/out  memory:  \", out)\n",
        "  \n",
        "print(inputs)\n",
        "print(torch.cat(inputs, dim=1))\n",
        "print(torch.cat(inputs, dim=0))\n",
        "\n",
        "# instead of for loop we can do this\n",
        "hidden = original_hidden\n",
        "out, hidden = lstm(torch.cat(inputs, dim=0).view(len(inputs), 1, -1), hidden)\n",
        "print(out)\n",
        "print(hidden)\n",
        "print(\"comparing preivous output with this output should clear things out\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM(3, 4)\n",
            "[tensor([[0.6614, 0.2669, 0.0617]]), tensor([[ 0.6213, -0.4519, -0.1661]]), tensor([[-1.5228,  0.3817, -1.0276]]), tensor([[-0.5631, -0.8923, -0.0583]]), tensor([[-0.1955, -0.9656,  0.4224]])]\n",
            "(tensor([[[ 0.2673, -0.4212, -0.5107, -1.5727]]]), tensor([[[-0.1232,  3.5870, -1.8313,  1.5987]]]))\n",
            "short+long memory (tensor([[[-0.1627,  0.3379, -0.4808,  0.3262]]], grad_fn=<StackBackward>), tensor([[[-0.3569,  2.4609, -0.6867,  0.6246]]], grad_fn=<StackBackward>))\n",
            "short/out  memory:   tensor([[[-0.1627,  0.3379, -0.4808,  0.3262]]], grad_fn=<StackBackward>)\n",
            "short+long memory (tensor([[[-0.1496,  0.5506, -0.3423,  0.0749]]], grad_fn=<StackBackward>), tensor([[[-0.2599,  1.6627, -0.4990,  0.1304]]], grad_fn=<StackBackward>))\n",
            "short/out  memory:   tensor([[[-0.1496,  0.5506, -0.3423,  0.0749]]], grad_fn=<StackBackward>)\n",
            "short+long memory (tensor([[[-0.2471,  0.1699, -0.0382,  0.2335]]], grad_fn=<StackBackward>), tensor([[[-0.6025,  0.6072, -0.0659,  0.5003]]], grad_fn=<StackBackward>))\n",
            "short/out  memory:   tensor([[[-0.2471,  0.1699, -0.0382,  0.2335]]], grad_fn=<StackBackward>)\n",
            "short+long memory (tensor([[[-0.2795,  0.1334, -0.1394,  0.1713]]], grad_fn=<StackBackward>), tensor([[[-0.5712,  0.3054, -0.2005,  0.2799]]], grad_fn=<StackBackward>))\n",
            "short/out  memory:   tensor([[[-0.2795,  0.1334, -0.1394,  0.1713]]], grad_fn=<StackBackward>)\n",
            "short+long memory (tensor([[[-0.2240,  0.1006, -0.2225,  0.0499]]], grad_fn=<StackBackward>), tensor([[[-0.4681,  0.1999, -0.3077,  0.0813]]], grad_fn=<StackBackward>))\n",
            "short/out  memory:   tensor([[[-0.2240,  0.1006, -0.2225,  0.0499]]], grad_fn=<StackBackward>)\n",
            "[tensor([[0.6614, 0.2669, 0.0617]]), tensor([[ 0.6213, -0.4519, -0.1661]]), tensor([[-1.5228,  0.3817, -1.0276]]), tensor([[-0.5631, -0.8923, -0.0583]]), tensor([[-0.1955, -0.9656,  0.4224]])]\n",
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519, -0.1661, -1.5228,  0.3817,\n",
            "         -1.0276, -0.5631, -0.8923, -0.0583, -0.1955, -0.9656,  0.4224]])\n",
            "tensor([[ 0.6614,  0.2669,  0.0617],\n",
            "        [ 0.6213, -0.4519, -0.1661],\n",
            "        [-1.5228,  0.3817, -1.0276],\n",
            "        [-0.5631, -0.8923, -0.0583],\n",
            "        [-0.1955, -0.9656,  0.4224]])\n",
            "tensor([[[-0.1627,  0.3379, -0.4808,  0.3262]],\n",
            "\n",
            "        [[-0.1496,  0.5506, -0.3423,  0.0749]],\n",
            "\n",
            "        [[-0.2471,  0.1699, -0.0382,  0.2335]],\n",
            "\n",
            "        [[-0.2795,  0.1334, -0.1394,  0.1713]],\n",
            "\n",
            "        [[-0.2240,  0.1006, -0.2225,  0.0499]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.2240,  0.1006, -0.2225,  0.0499]]], grad_fn=<StackBackward>), tensor([[[-0.4681,  0.1999, -0.3077,  0.0813]]], grad_fn=<StackBackward>))\n",
            "comparing preivous output with this output should clear things out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q462VSCu0GG",
        "colab_type": "text"
      },
      "source": [
        "#LSTM for part of speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBoI2b3YFZFi",
        "colab_type": "text"
      },
      "source": [
        "##Data Prepration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l9i1bnZuuML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "43b63882-4e31-4b54-8dd0-f61d727bd28c"
      },
      "source": [
        "training_data = [\n",
        "    (\"My name is bipin oli\".split(), [\"Pronoun\", \"Noun\", \"Verb\", \"Noun\", \"Noun\"]),\n",
        "    (\"Buddha was born in Nepal\".split(), [\"Noun\", \"Verb\", \"Verb\", \"Preposition\", \"Noun\"]),\n",
        "    (\"A child is a father of a man\".split(), [\"Determiner\", \"Noun\", \"Verb\", \"Determiner\", \"Noun\", \"Preposition\", \"Determiner\", \"Noun\"])\n",
        "]\n",
        "\n",
        "print(training_data)\n",
        "\n",
        "# construct vocubulary\n",
        "words_vocab = {}\n",
        "pos_vocab = {}\n",
        "pos_names = {}\n",
        "\n",
        "for row in training_data:\n",
        "  for word in row[0]:\n",
        "    if word.lower() not in words_vocab:\n",
        "      words_vocab[word.lower()] = len(words_vocab)\n",
        "  for pos in row[1]:\n",
        "    if pos not in pos_vocab:\n",
        "      pos_vocab[pos] = len(pos_vocab)\n",
        "      pos_names[pos_vocab[pos]] = pos\n",
        "      \n",
        "print(words_vocab)\n",
        "print(pos_vocab)\n",
        "print(pos_names)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['My', 'name', 'is', 'bipin', 'oli'], ['Pronoun', 'Noun', 'Verb', 'Noun', 'Noun']), (['Buddha', 'was', 'born', 'in', 'Nepal'], ['Noun', 'Verb', 'Verb', 'Preposition', 'Noun']), (['A', 'child', 'is', 'a', 'father', 'of', 'a', 'man'], ['Determiner', 'Noun', 'Verb', 'Determiner', 'Noun', 'Preposition', 'Determiner', 'Noun'])]\n",
            "{'my': 0, 'name': 1, 'is': 2, 'bipin': 3, 'oli': 4, 'buddha': 5, 'was': 6, 'born': 7, 'in': 8, 'nepal': 9, 'a': 10, 'child': 11, 'father': 12, 'of': 13, 'man': 14}\n",
            "{'Pronoun': 0, 'Noun': 1, 'Verb': 2, 'Preposition': 3, 'Determiner': 4}\n",
            "{0: 'Pronoun', 1: 'Noun', 2: 'Verb', 3: 'Preposition', 4: 'Determiner'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgfRbPPMFcaw",
        "colab_type": "text"
      },
      "source": [
        "##LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbY19Zf4Fek5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "  \n",
        "  def __init__(self, words_vocab_size, pos_vocab_size, word_vector_dim):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    \n",
        "    self.wordEmbeds = nn.Embedding(words_vocab_size, word_vector_dim)\n",
        "    self.lstm = nn.LSTM(input_size = word_vector_dim, hidden_size = word_vector_dim)\n",
        "    self.hidden2tag = nn.Linear(word_vector_dim, pos_vocab_size)\n",
        "    \n",
        "    \n",
        "  def forward(self, sentence): # sentence is a list of words\n",
        "    # it may seem that this network is not using any non linear activation \n",
        "    # funcations but inside LSTM there are sigmoid ans tanh activations \n",
        "    embeds = self.wordEmbeds(sentence)\n",
        "    outs, hidden_state = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    tags = self.hidden2tag(outs.view(len(sentence), -1))\n",
        "    tags_score = F.log_softmax(tags, dim=1)\n",
        "    return tags_score  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBfiahvpI9Oy",
        "colab_type": "text"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxSPSqUCI_h5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "e5bac9c1-cad9-47b2-ae7f-1d66670875a2"
      },
      "source": [
        "model = LSTMTagger(len(words_vocab), len(pos_vocab), word_vector_dim = 7)\n",
        "loss_func = nn.NLLLoss() # it expects log_porbabilites and class_values as input\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "print(\"parameters to learn value of:\")\n",
        "for name, param in model.named_parameters():\n",
        "  print(name)\n",
        "\n",
        "\n",
        "for epoch in range(300): # just going overboard with toy data\n",
        "  for data in training_data:\n",
        "    sentence = torch.tensor([words_vocab[word.lower()] for word in data[0]], dtype=torch.long)\n",
        "    pos = torch.tensor([pos_vocab[p] for p in data[1]], dtype=torch.long)\n",
        "\n",
        "    model.zero_grad()\n",
        "    pos_scores = model(sentence)\n",
        "    loss = loss_func(pos_scores, pos)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    "
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters to learn value of:\n",
            "wordEmbeds.weight\n",
            "lstm.weight_ih_l0\n",
            "lstm.weight_hh_l0\n",
            "lstm.bias_ih_l0\n",
            "lstm.bias_hh_l0\n",
            "hidden2tag.weight\n",
            "hidden2tag.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASxK03yjRhsN",
        "colab_type": "text"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoNg7jCNRjeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f282a1d3-a7d4-41a7-b736-901292602ef5"
      },
      "source": [
        "with torch.no_grad():\n",
        "  \n",
        "  # it is not made to work with unseen words\n",
        "  test_data = (\"Buddha was a child of a man\").split()\n",
        "  \n",
        "  sentence = torch.tensor([words_vocab[word.lower()] for word in test_data], dtype=torch.long)\n",
        "  \n",
        "  pred_scores = model(sentence)\n",
        "  _, pred_indexes = torch.max(pred_scores, dim=1) # across dim 1\n",
        "  \n",
        "  print(\"Tagging\")\n",
        "  for word in test_data:\n",
        "    print(word, end=\" \")\n",
        "  print()\n",
        "  for index in pred_indexes:\n",
        "    print(pos_names[index.item()], end=\" \")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tagging\n",
            "Buddha was a child of a man \n",
            "Noun Verb Determiner Noun Preposition Determiner Noun "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}